{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e761f47c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.18, Python 3.9.7)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from rl.common.logger import ConsoleLogger, FigureLogger, Tracker\n",
    "from rl.ppo.policies import ActorCriticNet\n",
    "from rl.ppo.ppo import PPO\n",
    "\n",
    "from torch import optim\n",
    "from env import VanillaEnv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3a0582f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_obstacle_pos = 26 # how many obstacle position you want to try out (paper: 27, max: 30)\n",
    "n_floor_heights = 11 # how many floor heights you want to try out (paper: 11, max: 40)\n",
    "obstacle_pos = np.rint(np.linspace(VanillaEnv.min_obstacle_pos, VanillaEnv.max_obstacle_pos, n_obstacle_pos)).astype(np.int8)\n",
    "floor_height = np.rint(np.linspace(VanillaEnv.min_floor_height, VanillaEnv.max_floor_height, n_floor_heights)).astype(np.int8)\n",
    "\n",
    "ALL_CONFIGURATIONS = set(itertools.product(obstacle_pos, floor_height))\n",
    "\n",
    "grid = np.zeros((len(obstacle_pos), len(floor_height)))\n",
    "\n",
    "train_conf = {\n",
    "    \"narrow_grid\": set([\n",
    "        # (obstacle_pos, floor_height)\n",
    "        (26, 12), (29, 12), (31, 12), (34, 12),\n",
    "        (26, 20), (29, 20), (31, 20), (34, 20),\n",
    "        (26, 28), (29, 28), (31, 28), (34, 28),\n",
    "    ]),\n",
    "    \"wide_grid\": set([\n",
    "        # (obstacle_pos, floor_height)\n",
    "        (22,  8), (27,  8), (32,  8), (38,  8),\n",
    "        (22, 20), (27, 20), (32, 20), (38, 20),\n",
    "        (22, 32), (27, 32), (32, 32), (38, 32),\n",
    "    ]),\n",
    "    \"random_grid\": set([\n",
    "        # (obstacle_pos, floor_height)\n",
    "        (15, 36), (17,  8), (19, 20), (21, 32),\n",
    "        (26, 20), (30,  4), (32, 24), (34, 36),\n",
    "        (36,  4), (38, 16), (43, 12), (44, 28),\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# just quickly check that all training configurations are valid\n",
    "for conf_name in train_conf.keys():\n",
    "    for conf in train_conf[conf_name]:\n",
    "        assert conf in ALL_CONFIGURATIONS, f\"Invalid configuration in {conf_name}\"\n",
    "\n",
    "# TEST_CONFIGURATIONS = ALL_CONFIGURATIONS - TRAINING_CONFIGURATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12fc73a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Training on narrow_grid ======\n",
      "Episode:   2500, avg.ret.: 73.6900 (over last 100 episodes)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m tracker \u001b[39m=\u001b[39m Tracker(logger1, logger2)\n\u001b[0;32m     13\u001b[0m ppo \u001b[39m=\u001b[39m PPO(policy, env, optimizer, seed\u001b[39m=\u001b[39m\u001b[39m31\u001b[39m, tracker\u001b[39m=\u001b[39mtracker)\n\u001b[1;32m---> 14\u001b[0m ppo\u001b[39m.\u001b[39;49mlearn(episodes)\n\u001b[0;32m     15\u001b[0m ppo\u001b[39m.\u001b[39msave(\u001b[39m'\u001b[39m\u001b[39m./ckpts\u001b[39m\u001b[39m'\u001b[39m, conf_name, info\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mconf\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mlist\u001b[39m(train_conf[conf_name])})\n\u001b[0;32m     17\u001b[0m fig \u001b[39m=\u001b[39m logger2\u001b[39m.\u001b[39mget_figure(fig_size\u001b[39m=\u001b[39m(\u001b[39m8\u001b[39m, \u001b[39m4\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\manus\\code\\python\\jku_practical_work\\rl\\ppo\\ppo.py:150\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, n_episodes)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer) \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer\u001b[39m.\u001b[39mmin_transitions: \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39m# update the network\u001b[39;00m\n\u001b[1;32m--> 150\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m    151\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer\u001b[39m.\u001b[39mupdate_stats()\n\u001b[0;32m    152\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_buffer_reset: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer\u001b[39m.\u001b[39mreset()\n",
      "File \u001b[1;32mc:\\Users\\manus\\code\\python\\jku_practical_work\\rl\\ppo\\ppo.py:106\u001b[0m, in \u001b[0;36mPPO.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39m# perform training step\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m--> 106\u001b[0m loss\u001b[39m.\u001b[39;49mmean()\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m    107\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m    108\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtracker\u001b[39m.\u001b[39mend_epoch()\n",
      "File \u001b[1;32mc:\\Users\\manus\\.conda\\envs\\drl-jumping\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\manus\\.conda\\envs\\drl-jumping\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "episodes = 10_000\n",
    "for conf_name in train_conf.keys():\n",
    "    print(f\"====== Training on {conf_name} ======\")\n",
    "    env = VanillaEnv(list(train_conf[conf_name]))\n",
    "\n",
    "    policy: ActorCriticNet = ActorCriticNet()\n",
    "    optimizer = optim.Adam(policy.parameters(), lr=0.001)\n",
    "\n",
    "    logger1 = ConsoleLogger(log_every=2500, average_over=100)\n",
    "    logger2 = FigureLogger()\n",
    "    tracker = Tracker(logger1, logger2)\n",
    "\n",
    "    ppo = PPO(policy, env, optimizer, seed=31, tracker=tracker)\n",
    "    ppo.learn(episodes)\n",
    "    ppo.save('./ckpts', conf_name, info={'conf': list(train_conf[conf_name])})\n",
    "\n",
    "    fig = logger2.get_figure(fig_size=(8, 4))\n",
    "    fig.suptitle(f\"Training on {conf_name} for {episodes} episodes\")\n",
    "    plt.show()\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
